{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZwJtEqfapVR",
        "outputId": "09fcacfe-a552-46eb-a555-7a7ea8110c6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.23.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.2\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.30.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install transformers\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YS9c91AKa95m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datasets\n",
        "import evaluate\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoModel\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zo9GIvzqhOv"
      },
      "outputs": [],
      "source": [
        "# nome modello\n",
        "model_name = 'dbmdz/bert-base-italian-cased'\n",
        "\n",
        "# carico il modello\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# tokenizzatore associato\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFLQ3yqRsgN0"
      },
      "outputs": [],
      "source": [
        "# carico i dati già tokenizzati estratti nel notebook \"bert_fine_tuning_coherence_tedx\"\n",
        "train = datasets.load_from_disk('/content/drive/MyDrive/Colab Notebooks/hf_tokenized_train')\n",
        "val = datasets.load_from_disk('/content/drive/MyDrive/Colab Notebooks/hf_tokenized_val')\n",
        "test = datasets.load_from_disk('/content/drive/MyDrive/Colab Notebooks/hf_tokenized_test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYXvSBmse7ux"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrtP1QcouxIK"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPlVJ9kOWo-g"
      },
      "outputs": [],
      "source": [
        "# estraggo i dati sui quali ho usato BERT senza fine-tuning per fare feature extraction, mi servono per pre-addestrare la testa di classificazione\n",
        "X_train = np.load('/content/drive/My Drive/feat_ex_data.npz')['X_train']\n",
        "y_train = np.load('/content/drive/My Drive/feat_ex_data.npz')['y_train']\n",
        "\n",
        "X_val = np.load('/content/drive/My Drive/feat_ex_data.npz')['X_val']\n",
        "y_val = np.load('/content/drive/My Drive/feat_ex_data.npz')['y_val']\n",
        "\n",
        "X_test = np.load('/content/drive/My Drive/feat_ex_data.npz')['X_test']\n",
        "y_test = np.load('/content/drive/My Drive/feat_ex_data.npz')['y_test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIlowPCPXRdb",
        "outputId": "64b258ac-f377-42c8-fd75-eba5b2492569"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((7200, 768), (800, 768), (800, 768))"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape, X_val.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "832ZjQyB1Jhm"
      },
      "outputs": [],
      "source": [
        "class MLP(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Testa di classificazione personalizzata\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "\n",
        "    # layer denso 1\n",
        "    self.dense1 = nn.Linear(768, 128)\n",
        "    # layer denso 2\n",
        "    self.dense2 = nn.Linear(128, 256)\n",
        "\n",
        "    # layer denso 3\n",
        "    self.dense3 = nn.Linear(256, 512)\n",
        "\n",
        "    # layer denso 4\n",
        "    self.dense4 = nn.Linear(512, 128)\n",
        "\n",
        "    # layer di output\n",
        "    self.out = nn.Linear(128,1)\n",
        "\n",
        "    # dropout\n",
        "    self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    # batchnorm\n",
        "    self.batchnorm1 = nn.BatchNorm1d(128)\n",
        "    self.batchnorm2 = nn.BatchNorm1d(256)\n",
        "    self.batchnorm3 = nn.BatchNorm1d(512)\n",
        "    self.batchnorm4 = nn.BatchNorm1d(128)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # primo layer denso\n",
        "    x = self.dense1(x)\n",
        "    x = self.batchnorm1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    # per la connessione residua\n",
        "    x1 = x.clone()\n",
        "\n",
        "\n",
        "    # secondo layer denso\n",
        "    x = self.dense2(x)\n",
        "    x = self.batchnorm2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    # terzo layer denso\n",
        "    x = self.dense3(x)\n",
        "    x = self.batchnorm3(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    # quarto layer denso\n",
        "    x = self.dense4(x)\n",
        "    x = self.batchnorm4(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "\n",
        "    # layer di output\n",
        "    x = self.out(x + x1) # connessione residua\n",
        "    x = F.sigmoid(x).squeeze()\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQpWYkPE_lTs",
        "outputId": "4fe15782-15c1-4b34-856a-baf6bd527c59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (dense1): Linear(in_features=768, out_features=128, bias=True)\n",
              "  (dense2): Linear(in_features=128, out_features=256, bias=True)\n",
              "  (dense25): Linear(in_features=256, out_features=512, bias=True)\n",
              "  (dense3): Linear(in_features=512, out_features=128, bias=True)\n",
              "  (out): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (batchnorm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (batchnorm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (batchnorm25): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (batchnorm3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              ")"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mlp = MLP()\n",
        "mlp.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    \"\"\"\n",
        "    Modello completo usato per il fine-tuning, con BERT e la testa di classificazione personalizzata.\n",
        "    L'input e l'output del metodo \"forward\" seguono l'interfaccia stardard di HuggingFace, in modo da poter usare il trainer di HuggingFace per il fine-tuning\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.bert = model\n",
        "        self.mlp = mlp\n",
        "\n",
        "        self.criterion = nn.BCELoss()\n",
        "\n",
        "    def forward(self, input_ids, labels, attention_mask):\n",
        "      x = self.bert(input_ids, attention_mask).pooler_output\n",
        "\n",
        "      logits = torch.Tensor(self.mlp(x))\n",
        "      loss = self.criterion(logits, labels.float())\n",
        "      return {'loss': loss, 'logits': logits}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PKnVqJwAKZ6"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "# trasformo i dataset della features extraction in tensori torch\n",
        "X_train_t = torch.Tensor(X_train)\n",
        "X_val_t = torch.Tensor(X_val).to(device)\n",
        "X_test_t = torch.Tensor(X_test)\n",
        "\n",
        "y_train_t = torch.Tensor(y_train).float()\n",
        "y_val_t = torch.Tensor(y_val).float().to(device)\n",
        "y_test_t = torch.Tensor(y_test).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_EUmWIoAkUX"
      },
      "outputs": [],
      "source": [
        "# trasformo train e test in DataLoader\n",
        "data_train = TensorDataset(X_train_t, y_train_t)\n",
        "data_test = TensorDataset(X_test_t, y_test_t)\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(data_train, batch_size=256, shuffle=True, num_workers=2)\n",
        "test_dataloader = DataLoader(data_test, batch_size=1, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nra-NBStC5dX"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(y_true, y_prob):\n",
        "    assert y_true.ndim == 1 and y_true.size() == y_prob.size()\n",
        "    y_prob = y_prob > 0.5\n",
        "    return (y_true == y_prob).sum().item() / y_true.size(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkHt4n7_IMn6"
      },
      "outputs": [],
      "source": [
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "val_loss_list = []\n",
        "val_acc_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBonCuHWINh_"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFT-xPOP_qrZ",
        "outputId": "67771555-4d3e-4412-b0f6-27ee1c7c25a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoca: 1 | t: 0.27 sec || train_loss: 0.7393379889685532 | train_acc: 0.5080818965517241 || val_loss: 0.7283404469490051 | val_acc: 0.515\n",
            "epoca: 2 | t: 0.26 sec || train_loss: 0.7220353114193884 | train_acc: 0.5119881465517241 || val_loss: 0.6875580549240112 | val_acc: 0.57625\n",
            "epoca: 3 | t: 0.27 sec || train_loss: 0.7041510179125029 | train_acc: 0.5294989224137931 || val_loss: 0.6984705328941345 | val_acc: 0.5225\n",
            "epoca: 4 | t: 0.26 sec || train_loss: 0.6973651236501234 | train_acc: 0.5367726293103449 || val_loss: 0.7084214687347412 | val_acc: 0.51\n",
            "epoca: 5 | t: 0.27 sec || train_loss: 0.6950311003060177 | train_acc: 0.5405441810344828 || val_loss: 0.6818770170211792 | val_acc: 0.55125\n",
            "epoca: 6 | t: 0.26 sec || train_loss: 0.69486193821348 | train_acc: 0.5387931034482759 || val_loss: 0.6906672716140747 | val_acc: 0.53125\n",
            "epoca: 7 | t: 0.27 sec || train_loss: 0.6915096969440065 | train_acc: 0.5405441810344828 || val_loss: 0.682951033115387 | val_acc: 0.545\n",
            "epoca: 8 | t: 0.26 sec || train_loss: 0.6855829275887588 | train_acc: 0.5577855603448276 || val_loss: 0.6885741949081421 | val_acc: 0.565\n",
            "epoca: 9 | t: 0.26 sec || train_loss: 0.6850856583693932 | train_acc: 0.5614224137931034 || val_loss: 0.6825366616249084 | val_acc: 0.55375\n",
            "epoca: 10 | t: 0.27 sec || train_loss: 0.6834362108131935 | train_acc: 0.556707974137931 || val_loss: 0.6853932738304138 | val_acc: 0.5575\n",
            "epoca: 11 | t: 0.26 sec || train_loss: 0.6861549615859985 | train_acc: 0.5577855603448276 || val_loss: 0.6871258616447449 | val_acc: 0.55375\n",
            "epoca: 12 | t: 0.26 sec || train_loss: 0.6821895837783813 | train_acc: 0.5584590517241379 || val_loss: 0.6757914423942566 | val_acc: 0.585\n",
            "epoca: 13 | t: 0.28 sec || train_loss: 0.6824220994423176 | train_acc: 0.5495689655172413 || val_loss: 0.683161735534668 | val_acc: 0.55875\n",
            "epoca: 14 | t: 0.26 sec || train_loss: 0.6814281714373621 | train_acc: 0.5674838362068966 || val_loss: 0.6748893857002258 | val_acc: 0.58625\n",
            "epoca: 15 | t: 0.28 sec || train_loss: 0.6788671962146101 | train_acc: 0.5618265086206896 || val_loss: 0.6790873408317566 | val_acc: 0.58\n",
            "epoca: 16 | t: 0.28 sec || train_loss: 0.6785348674346661 | train_acc: 0.5732758620689655 || val_loss: 0.6716248989105225 | val_acc: 0.58\n",
            "epoca: 17 | t: 0.27 sec || train_loss: 0.6758344687264541 | train_acc: 0.5736799568965517 || val_loss: 0.6878103613853455 | val_acc: 0.57375\n",
            "epoca: 18 | t: 0.27 sec || train_loss: 0.6818891866453762 | train_acc: 0.5665409482758621 || val_loss: 0.6788032650947571 | val_acc: 0.5875\n",
            "epoca: 19 | t: 0.26 sec || train_loss: 0.6759604380048555 | train_acc: 0.5727370689655172 || val_loss: 0.6739088296890259 | val_acc: 0.58375\n",
            "epoca: 20 | t: 0.27 sec || train_loss: 0.6778159675926998 | train_acc: 0.5758351293103449 || val_loss: 0.673783540725708 | val_acc: 0.585\n",
            "epoca: 21 | t: 0.27 sec || train_loss: 0.6793892404128765 | train_acc: 0.5627693965517241 || val_loss: 0.6882687211036682 | val_acc: 0.53125\n",
            "epoca: 22 | t: 0.29 sec || train_loss: 0.6768544419058438 | train_acc: 0.5731411637931034 || val_loss: 0.6822239756584167 | val_acc: 0.57\n",
            "epoca: 23 | t: 0.27 sec || train_loss: 0.6732223054458355 | train_acc: 0.5808189655172413 || val_loss: 0.6803166270256042 | val_acc: 0.565\n",
            "epoca: 24 | t: 0.27 sec || train_loss: 0.6749971241786562 | train_acc: 0.5771821120689655 || val_loss: 0.6671469807624817 | val_acc: 0.6075\n",
            "epoca: 25 | t: 0.26 sec || train_loss: 0.674633015846384 | train_acc: 0.5727370689655172 || val_loss: 0.6741392612457275 | val_acc: 0.58625\n",
            "epoca: 26 | t: 0.26 sec || train_loss: 0.6782790073033037 | train_acc: 0.5635775862068966 || val_loss: 0.6760194301605225 | val_acc: 0.58\n",
            "epoca: 27 | t: 0.26 sec || train_loss: 0.6744990862649063 | train_acc: 0.5730064655172413 || val_loss: 0.6733158826828003 | val_acc: 0.595\n",
            "epoca: 28 | t: 0.26 sec || train_loss: 0.6725466169159988 | train_acc: 0.5844558189655172 || val_loss: 0.6756390333175659 | val_acc: 0.57125\n",
            "epoca: 29 | t: 0.26 sec || train_loss: 0.6749899716212832 | train_acc: 0.5789331896551724 || val_loss: 0.6772695779800415 | val_acc: 0.58625\n",
            "epoca: 30 | t: 0.27 sec || train_loss: 0.6720235471067757 | train_acc: 0.5886314655172413 || val_loss: 0.6730297803878784 | val_acc: 0.5525\n",
            "epoca: 31 | t: 0.26 sec || train_loss: 0.6711784621764874 | train_acc: 0.587957974137931 || val_loss: 0.674313485622406 | val_acc: 0.5625\n",
            "epoca: 32 | t: 0.26 sec || train_loss: 0.6735459257816446 | train_acc: 0.5719288793103449 || val_loss: 0.6729404926300049 | val_acc: 0.57125\n",
            "epoca: 33 | t: 0.28 sec || train_loss: 0.6712069367540294 | train_acc: 0.583917025862069 || val_loss: 0.6778764128684998 | val_acc: 0.56875\n",
            "epoca: 34 | t: 0.26 sec || train_loss: 0.6667160987854004 | train_acc: 0.5930765086206896 || val_loss: 0.6755679845809937 | val_acc: 0.58875\n",
            "epoca: 35 | t: 0.27 sec || train_loss: 0.6707820501820795 | train_acc: 0.5894396551724138 || val_loss: 0.6727585196495056 | val_acc: 0.5825\n"
          ]
        }
      ],
      "source": [
        "# loss function e ottimizzatore\n",
        "loss = torch.nn.BCELoss() \n",
        "opt = torch.optim.Adam(mlp.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "\n",
        "n_epochs = 35   # numero di epoche\n",
        "batch_size = 256  # size of each batch\n",
        "batches_per_epoch = X_train.shape[0] // batch_size\n",
        "\n",
        "\n",
        "time_ = time.time()\n",
        "delta_time = 0\n",
        "\n",
        "best_acc = 0\n",
        "patience = 0 # patience per il criterio di early stopping\n",
        "\n",
        "\n",
        "# TRAINING LOOP del preaddestramento della testa di classificazione\n",
        "for epoch in range(n_epochs):\n",
        "  mlp.train()\n",
        "\n",
        "  running_loss_val = 0.0\n",
        "  running_acc = 0.0\n",
        "  for i, data in enumerate(train_dataloader):\n",
        "    # sposto i batch sulla gpu\n",
        "    X = data[0].to(device)\n",
        "    y = data[1].to(device)\n",
        "\n",
        "    # azzero il gradiente\n",
        "    opt.zero_grad()\n",
        "\n",
        "    #forward\n",
        "    y_pred = mlp(X)\n",
        "\n",
        "    # calcolo training loss\n",
        "    loss_val = loss(y_pred, y)\n",
        "\n",
        "\n",
        "    # training accuracy\n",
        "    train_acc = get_accuracy(y, y_pred)\n",
        "\n",
        "    # backprop\n",
        "    loss_val.backward()\n",
        "\n",
        "    # optimization\n",
        "    opt.step()\n",
        "\n",
        "\n",
        "    # statistiche\n",
        "    running_loss_val += loss_val.data.item()\n",
        "    running_acc += train_acc\n",
        "\n",
        "\n",
        "  #forward su validazione ogni fine epoca\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    y_pred_val = mlp(X_val_t)\n",
        "  val_loss_val = loss(y_pred_val, y_val_t)\n",
        "  val_acc = get_accuracy(y_val_t, y_pred_val)\n",
        "\n",
        "\n",
        "  delta_time = time.time() - time_\n",
        "  time_ = time.time()\n",
        "  print(f'epoca: {epoch+1} | t: {round(delta_time, 2)} sec || train_loss: {running_loss_val/(i+1)} | train_acc: {running_acc/(i+1)} || val_loss: {val_loss_val} | val_acc: {val_acc}' )\n",
        "\n",
        "  train_loss_list.append(running_loss_val/(i+1))\n",
        "  train_acc_list.append((running_acc/(i+1)))\n",
        "  val_loss_list.append(val_loss_val.item())\n",
        "  val_acc_list.append(val_acc)\n",
        "\n",
        "\n",
        "  # early stopping\n",
        "  if val_acc > best_acc:\n",
        "    best_acc = val_acc\n",
        "    patience = 0\n",
        "  else:\n",
        "    patience += 1\n",
        "    if patience == 100:\n",
        "      print('BLOCCATO')\n",
        "      break\n",
        "\n",
        "\n",
        "  running_loss_val = 0.0\n",
        "  running_acc = 0.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w469pFL0LhI5"
      },
      "outputs": [],
      "source": [
        "# definisco il modello completo\n",
        "net = Net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYa6e8QKwM7E",
        "outputId": "26fa30fb-907e-4a6c-fda2-bb7f7988bac4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "num_epochs = 5\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    logging_steps=10,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=num_epochs,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r0PvlyzBFSE"
      },
      "outputs": [],
      "source": [
        "def acc_metric(eval_pred):\n",
        "  accuracy_metric = evaluate.load(\"accuracy\")\n",
        "  predictions, labels = eval_pred\n",
        "  predictions = predictions >= 0.5\n",
        "  print((predictions == labels).sum() / labels.shape[0])\n",
        "\n",
        "  return accuracy_metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "f2Q0otF-_1-c",
        "outputId": "55f069ac-824f-40a4-8b4e-65c29d88ab70"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2250/2250 20:47, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.687100</td>\n",
              "      <td>0.614953</td>\n",
              "      <td>0.665000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.584200</td>\n",
              "      <td>0.597185</td>\n",
              "      <td>0.702500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.489500</td>\n",
              "      <td>0.630314</td>\n",
              "      <td>0.678750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.415300</td>\n",
              "      <td>0.632697</td>\n",
              "      <td>0.688750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.363400</td>\n",
              "      <td>0.656265</td>\n",
              "      <td>0.682500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.665\n",
            "0.7025\n",
            "0.67875\n",
            "0.68875\n",
            "0.6825\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2250, training_loss=0.5078917439778646, metrics={'train_runtime': 1248.3776, 'train_samples_per_second': 28.837, 'train_steps_per_second': 1.802, 'total_flos': 0.0, 'train_loss': 0.5078917439778646, 'epoch': 5.0})"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# fine tuning del modello completo\n",
        "trainer = Trainer(\n",
        "    net,\n",
        "    training_args,\n",
        "    train_dataset=train,\n",
        "    eval_dataset=val,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=acc_metric\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Valutazione del modello"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In questo caso la valutazione è stata fatta sullo stesso notebook, perchè, modificando la struttura del modello, non è più possibile salvarlo attraverso il trainer, che lo salverebbe come se fosse un BERT normale. Per semplicità è stato quindi valutato qui."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yN3fI-PZybZU"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "t4C-jYuSyTgm",
        "outputId": "ea589142-c3af-43c1-e0cc-92dc0043b4ec"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.675\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.64      0.66       400\n",
            "           1       0.66      0.71      0.69       400\n",
            "\n",
            "    accuracy                           0.68       800\n",
            "   macro avg       0.68      0.68      0.67       800\n",
            "weighted avg       0.68      0.68      0.67       800\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# valutazione sul test set\n",
        "y_pred_prob = trainer.predict(test)\n",
        "y_test = test[\"label\"].tolist()\n",
        "y_pred = y_pred_prob.predictions >= 0.5\n",
        "\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-IsNrPkED-v"
      },
      "outputs": [],
      "source": [
        "log_history = trainer.state.log_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mabqVe5Qt_Zu"
      },
      "outputs": [],
      "source": [
        "# salvo i logs\n",
        "import json\n",
        "with open('/content/drive/My Drive/finetuned_model_CLS_MLP__logs.json', 'w') as f:\n",
        "    json.dump(log_history, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwIg-wFVuqtS"
      },
      "outputs": [],
      "source": [
        "# visti i problemi di salvataggio, questo modello è stato salvato attraverso lo state_dict torch\n",
        "torch.save(net.state_dict(), '/content/drive/My Drive/finetuned_model_CLS_MLP')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
